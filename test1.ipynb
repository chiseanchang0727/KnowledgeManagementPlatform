{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from app.utils.data_loader import load_csv\n",
    "import re\n",
    "from configs import Configs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import  TensorDataset, Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(Configs.HISTORICAL_DATA_DIR, 'Bleached Softwood Kraft Pulp Futures Historical Data.csv')\n",
    "data = load_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.usegpu = True\n",
    "        self.seed = 42\n",
    "        self.batch_size = 300\n",
    "        self.lr = 1e-3\n",
    "        self.n_hidden = [16, 32, 32, 16]\n",
    "        self.epochs = 100\n",
    "        self.N_fold = 5\n",
    "        self.weight_decay = 1e-4\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product(name='itemA', price=100, qty=2, amount=200)\n"
     ]
    }
   ],
   "source": [
    "# for defining the variable type\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Product:\n",
    "    name: str\n",
    "    price: int\n",
    "    qty: int\n",
    "\n",
    "    # withi field(init=False), amount is not required when init\n",
    "    amount: int = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.amount = self.price * self.qty\n",
    "\n",
    "itemA = Product(name='itemA', price=100, qty=2)\n",
    "print(itemA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deivce = torch.device('cuda' if torch.cuda.is_available() and config.usegpu else 'cpu' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data_type(input: pd.DataFrame):\n",
    "\n",
    "    df = input.copy()\n",
    "    cols = ['Price', 'Open', 'High', 'Low', 'Vol.', 'Change %']\n",
    "\n",
    "    for col in cols:\n",
    "        if col == 'Vol.':\n",
    "            df[col] = df[col].apply(lambda x: float(re.sub('K', '', x))*1000 if x != '-' else 0)\n",
    "        elif col == 'Change %':\n",
    "            df[col] = df[col].apply(lambda x: float(re.sub('%', '', x)) /100)\n",
    "        else:\n",
    "            df[col] = df[col].apply(lambda x: float(re.sub(',', '', x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(raw_data):\n",
    "    data = raw_data.copy()\n",
    "\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data['Year'] = data['Date'].dt.year\n",
    "    data['Month'] = data['Date'].dt.month\n",
    "    data['Day'] = data['Date'].dt.day\n",
    "\n",
    "    data = change_data_type(data)\n",
    "\n",
    "    data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "\n",
    "    data = data.drop('Date', axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Change %</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4744.0</td>\n",
       "      <td>4748.0</td>\n",
       "      <td>4752.0</td>\n",
       "      <td>4734.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4740.0</td>\n",
       "      <td>4738.0</td>\n",
       "      <td>4750.0</td>\n",
       "      <td>4730.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4706.0</td>\n",
       "      <td>4722.0</td>\n",
       "      <td>4734.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4704.0</td>\n",
       "      <td>4698.0</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4764.0</td>\n",
       "      <td>4718.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>4718.0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>5626.0</td>\n",
       "      <td>5616.0</td>\n",
       "      <td>5670.0</td>\n",
       "      <td>5612.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>5546.0</td>\n",
       "      <td>5602.0</td>\n",
       "      <td>5624.0</td>\n",
       "      <td>5502.0</td>\n",
       "      <td>2180.0</td>\n",
       "      <td>-0.0142</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>5536.0</td>\n",
       "      <td>5566.0</td>\n",
       "      <td>5582.0</td>\n",
       "      <td>5510.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-0.0018</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>5552.0</td>\n",
       "      <td>5532.0</td>\n",
       "      <td>5578.0</td>\n",
       "      <td>5526.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>5564.0</td>\n",
       "      <td>5580.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>5510.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1143 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Price    Open    High     Low    Vol.  Change %  Year  Month  Day\n",
       "0     4744.0  4748.0  4752.0  4734.0   170.0    0.0013  2020      1    2\n",
       "1     4740.0  4738.0  4750.0  4730.0   160.0   -0.0008  2020      1    3\n",
       "2     4706.0  4722.0  4734.0  4688.0   740.0   -0.0072  2020      1    6\n",
       "3     4704.0  4698.0  4720.0  4688.0   590.0   -0.0004  2020      1    7\n",
       "4     4764.0  4718.0  4800.0  4718.0  1520.0    0.0128  2020      1    8\n",
       "...      ...     ...     ...     ...     ...       ...   ...    ...  ...\n",
       "1138  5626.0  5616.0  5670.0  5612.0   360.0   -0.0004  2024      9   10\n",
       "1139  5546.0  5602.0  5624.0  5502.0  2180.0   -0.0142  2024      9   11\n",
       "1140  5536.0  5566.0  5582.0  5510.0   180.0   -0.0018  2024      9   12\n",
       "1141  5552.0  5532.0  5578.0  5526.0   310.0    0.0029  2024      9   13\n",
       "1142  5564.0  5580.0  5600.0  5510.0  1150.0    0.0022  2024      9   18\n",
       "\n",
       "[1143 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset: TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = data.drop('Price', axis=1)\n",
    "df_label = data['Price']\n",
    "\n",
    "features = df_features.columns\n",
    "label = ['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Change %</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4748.0</td>\n",
       "      <td>4752.0</td>\n",
       "      <td>4734.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4738.0</td>\n",
       "      <td>4750.0</td>\n",
       "      <td>4730.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4722.0</td>\n",
       "      <td>4734.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4698.0</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4718.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>4718.0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>5616.0</td>\n",
       "      <td>5670.0</td>\n",
       "      <td>5612.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>5602.0</td>\n",
       "      <td>5624.0</td>\n",
       "      <td>5502.0</td>\n",
       "      <td>2180.0</td>\n",
       "      <td>-0.0142</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>5566.0</td>\n",
       "      <td>5582.0</td>\n",
       "      <td>5510.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-0.0018</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>5532.0</td>\n",
       "      <td>5578.0</td>\n",
       "      <td>5526.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>5580.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>5510.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1143 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open    High     Low    Vol.  Change %  Year  Month  Day\n",
       "0     4748.0  4752.0  4734.0   170.0    0.0013  2020      1    2\n",
       "1     4738.0  4750.0  4730.0   160.0   -0.0008  2020      1    3\n",
       "2     4722.0  4734.0  4688.0   740.0   -0.0072  2020      1    6\n",
       "3     4698.0  4720.0  4688.0   590.0   -0.0004  2020      1    7\n",
       "4     4718.0  4800.0  4718.0  1520.0    0.0128  2020      1    8\n",
       "...      ...     ...     ...     ...       ...   ...    ...  ...\n",
       "1138  5616.0  5670.0  5612.0   360.0   -0.0004  2024      9   10\n",
       "1139  5602.0  5624.0  5502.0  2180.0   -0.0142  2024      9   11\n",
       "1140  5566.0  5582.0  5510.0   180.0   -0.0018  2024      9   12\n",
       "1141  5532.0  5578.0  5526.0   310.0    0.0029  2024      9   13\n",
       "1142  5580.0  5600.0  5510.0  1150.0    0.0022  2024      9   18\n",
       "\n",
       "[1143 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(df_features.to_numpy(), dtype=torch.float32)\n",
    "y = torch.tensor(df_label.to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset: Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df_input: pd.DataFrame, features: list, label: str, accelerator='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df_input (pd.DataFrame): input DataFrame\n",
    "            features (list): list of column names to use as features.\n",
    "            label (str): column name to use as the target.\n",
    "        \"\"\"\n",
    "        self.features = torch.FloatTensor(df_input[features].to_numpy()).to(accelerator)\n",
    "        self.label = torch.FloatTensor(df_input[label].to_numpy()).to(accelerator)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of data\n",
    "        \"\"\"\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieve one sample at the given index\n",
    "\n",
    "        Args:\n",
    "            idx(int): index of the sample to retrieve\n",
    "\n",
    "        Returns:\n",
    "            tuple(feature, target): as tensor\n",
    "        \"\"\"\n",
    "        features = self.features[index]\n",
    "        target = self.label[index]\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(nn.Module):\n",
    "    def __init__(self, df_train, batch_size, features, label, N_fold, accelerator):\n",
    "        super().__init__()\n",
    "        self.df = df_train\n",
    "        self.batch_size = batch_size\n",
    "        self.accelerator = accelerator\n",
    "        \n",
    "        # initial the datasets as None\n",
    "        self.tain_dataset = None\n",
    "        self.valid_dataset = None\n",
    "\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.N_fold = N_fold\n",
    "\n",
    "\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self, test_days=30):    \n",
    "        self.index_dict = {}\n",
    "        tss = TimeSeriesSplit(n_splits=self.N_fold, test_size=test_days)\n",
    "        for i, (train_idx, val_idx) in enumerate(tss.split(data)):\n",
    "            self.index_dict[i] = {\n",
    "                \"train_idx\": train_idx,\n",
    "                \"val_idx\": val_idx\n",
    "            }\n",
    "\n",
    "\n",
    "    def train_loader(self, fold, num_workers=0):\n",
    "        self.train_dataset = CustomDataset(\n",
    "            self.df[self.df.index.isin(self.index_dict[fold]['train_idx'])],\n",
    "            features=self.features,\n",
    "            label=self.label,\n",
    "            accelerator=self.accelerator\n",
    "        )\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    def valid_loader(self, fold, num_workers=0):\n",
    "        self.valid_dataset = CustomDataset(\n",
    "            self.df[self.df.index.isin(self.index_dict[fold]['val_idx'])],\n",
    "            features=self.features,\n",
    "            label=self.label,\n",
    "            accelerator=self.accelerator\n",
    "        )\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(df_train=data, batch_size=config.batch_size, features=features, label=label, N_fold=config.N_fold, accelerator=deivce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(config.N_fold):\n",
    "    data_module.train_loader(fold)\n",
    "    data_module.valid_loader(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.valid_dataset.features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(LightningModule):\n",
    "    def __init__(self, input_size, hidden_dims, lr, weight_decay, dropouts=None):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_dim = input_size\n",
    "\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.BatchNorm1d(input_dim))\n",
    "\n",
    "            if i > 0:\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "\n",
    "            # if i < len(dropouts):\n",
    "            #     layers.append(nn.Dropout(dropouts[i]))\n",
    "\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # now the input_dim is the final round of hidden layer\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1 means run one times\n",
    "        return self.model(x).squeeze(-1) * 1\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "\n",
    "        loss = self.criterion(y_pred, y)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_schedular': {\n",
    "                'schedular': schedular,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:377: Found unsupported keys in the optimizer configuration: {'lr_schedular'}\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model     | Sequential | 2.5 K  | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "2.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 43.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([300, 1])) that is different to the input size (torch.Size([300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 36.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:377: Found unsupported keys in the optimizer configuration: {'lr_schedular'}\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model     | Sequential | 2.5 K  | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "2.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 41.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([300, 1])) that is different to the input size (torch.Size([300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 38.33it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:377: Found unsupported keys in the optimizer configuration: {'lr_schedular'}\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model     | Sequential | 2.5 K  | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "2.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 52.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([300, 1])) that is different to the input size (torch.Size([300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 45.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:377: Found unsupported keys in the optimizer configuration: {'lr_schedular'}\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model     | Sequential | 2.5 K  | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "2.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 49.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([300, 1])) that is different to the input size (torch.Size([300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 43.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:377: Found unsupported keys in the optimizer configuration: {'lr_schedular'}\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model     | Sequential | 2.5 K  | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "2.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([300, 1])) that is different to the input size (torch.Size([300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 12.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for fold in range(config.N_fold):\n",
    "    \n",
    "    input_size = data_module.train_dataset.features.shape[1]\n",
    "\n",
    "    model = NN(\n",
    "        input_size=input_size,\n",
    "        hidden_dims=config.n_hidden,\n",
    "        lr=config.lr,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    test_trainer = Trainer(\n",
    "        fast_dev_run=True,\n",
    "    )\n",
    "\n",
    "    test_trainer.fit(\n",
    "        model,\n",
    "        data_module.train_loader(fold),\n",
    "        data_module.valid_loader(fold)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class LLMType(Enum):\n",
    "    \n",
    "    OpenAIChat = 'openai_chat'\n",
    "    AzureOpenAIChat = 'azure_openai_chat'\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        # return as a string, like \"azure_openai_chat\"\n",
    "        return f\"'{self.value}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azure_openai_chat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLMType.AzureOpenAIChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from llm.config.enum import LLMType\n",
    "\n",
    "class LLMParameter(BaseModel):\n",
    "    \"\"\"LLM Parameters model.\"\"\"\n",
    "    \n",
    "    type: LLMType = Field(\n",
    "        description=\"The type of LLM model to use.\", default=LLMType.AzureOpenAIChat\n",
    "    )\n",
    "    \n",
    "    api_key: str | None = Field(\n",
    "        description=\"The API key to use for the LLM service.\",\n",
    "        default_factory=lambda: os.getenv(\"OPENAI_API_KEY\"),  # Load from .env by default\n",
    "    )\n",
    "    \n",
    "    api_base: str | None = Field(\n",
    "        description=\"The base URL for the LLM API.\",\n",
    "        default_factory=lambda: os.getenv(\"OPENAI_API_BASE\"), \n",
    "    )\n",
    "    \n",
    "    api_version: str | None = Field(\n",
    "        description=\"The version of the LLM API to use.\",\n",
    "        default_factory=lambda: os.getenv(\"OPENAI_API_VERSION\"), \n",
    "    )\n",
    "    \n",
    "    deployment_name: str | None = Field(\n",
    "        description=\"The deployment name to use for the LLM service.\",\n",
    "        default_factory=lambda: os.getenv(\"OPENAI_DEPLOYMENT_NAME\"),  \n",
    "    )\n",
    "    \n",
    "    temperature: float | None = Field(\n",
    "        description=\"The temperature to use for token generation.\",\n",
    "        default=0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings(BaseSettings):\n",
    "    field_one: Optional[str]\n",
    "\n",
    "    model_config = SettingsConfigDict(env_file='local.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'field_one': 'one'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = Settings()\n",
    "settings.model_dump(mode='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLMParameter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_parameters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMParameters\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLLMConfig\u001b[39;00m(BaseModel):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Get a string representation.\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m, in \u001b[0;36mLLMConfig\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pformat(\u001b[38;5;28mself\u001b[39m, highlight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mtype\u001b[39m: LLMType \u001b[38;5;241m=\u001b[39m Field(\n\u001b[0;32m     12\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe type of LLM model to use\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39mLLMType\u001b[38;5;241m.\u001b[39mAzureOpenAIChat\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m llm: \u001b[43mLLMParameter\u001b[49m \u001b[38;5;241m=\u001b[39m Field(\n\u001b[0;32m     16\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe LLM configuration to use.\u001b[39m\u001b[38;5;124m\"\u001b[39m, default_factory\u001b[38;5;241m=\u001b[39mLLMParameters\n\u001b[0;32m     17\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LLMParameter' is not defined"
     ]
    }
   ],
   "source": [
    "from devtools import pformat\n",
    "from pydantic import BaseModel, Field\n",
    "from llm.config.enum import LLMType\n",
    "from llm.config.llm_parameters import LLMParameters\n",
    "import yaml\n",
    "class LLMConfig(BaseModel):\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Get a string representation.\"\"\"\n",
    "        return pformat(self, highlight=False)\n",
    "    \n",
    "    type: LLMType = Field(\n",
    "        description='The type of LLM model to use', default=LLMType.AzureOpenAIChat\n",
    "    )\n",
    "    \n",
    "    llm: LLMParameter = Field(\n",
    "        description=\"The LLM configuration to use.\", default_factory=LLMParameters\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def load_config_from_file(file_path: str) -> LLMConfig:\n",
    "    \"\"\"Load LLM configuration from a YAML file.\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        config_data = yaml.safe_load(file)\n",
    "    return LLMConfig(**config_data.get(\"llm\", {}))\n",
    "\n",
    "# Example usage\n",
    "config = load_config_from_file(\"test.yaml\")\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.config.enum import LLMType\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from llm.config.llm_config import LLMConfig\n",
    "\n",
    "\n",
    "class LLMBase:\n",
    "    def __init__(self, config: LLMConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def get_llm(self, llm_type=LLMType.AzureOpenAIChat):\n",
    "        if llm_type == LLMType.AzureOpenAIChat:\n",
    "            self.llm = self.get_aoai_llm()\n",
    "        else:\n",
    "            raise NotImplementedError(f\"LLM Type {llm_type} is not supported yet.\")\n",
    "        \n",
    "    def get_aoai_llm(self):\n",
    "\n",
    "        return AzureChatOpenAI(\n",
    "            azure_endpoint=self.config.llm.api_base,\n",
    "            openai_api_version=self.config.llm.api_version,\n",
    "            azure_deployment=self.config.llm.deployment_name,\n",
    "            openai_api_key=self.config.llm.api_key,\n",
    "            temperature=self.config.llm.temperature\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.bots.base import LLMBase\n",
    "from llm.config.llm_config import LLMConfig\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "class SummaryBot(LLMBase):\n",
    "    def __init__(self, config: LLMConfig):\n",
    "        super().__init__(config)\n",
    "        print(config.llm.api_base)\n",
    "        self.model = self.get_llm()\n",
    "\n",
    "    def summarize(self, input_string, prompt):\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\", prompt\n",
    "                ),\n",
    "                (\n",
    "                    \"human\",\"{input_string}\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        chain = (\n",
    "            {\"input_string\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | self.model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        return chain.invoke(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'llm.config.llm_config.LLMConfig'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "llm",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSummaryBot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLLMConfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mSummaryBot.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: LLMConfig):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241m.\u001b[39mapi_base)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_llm()\n",
      "File \u001b[1;32mc:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\kmp\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py:320\u001b[0m, in \u001b[0;36mModelMetaclass.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m private_attributes \u001b[38;5;129;01mand\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m private_attributes:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m private_attributes[item]\n\u001b[1;32m--> 320\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(item)\n",
      "\u001b[1;31mAttributeError\u001b[0m: llm"
     ]
    }
   ],
   "source": [
    "SummaryBot(config=LLMConfig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
