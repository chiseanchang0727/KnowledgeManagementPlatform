{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class LLMParameters(BaseModel):\n",
    "    \"\"\"LLM Parameters model.\"\"\"\n",
    "    \n",
    "    ############################ \n",
    "    # Load from .env by default\n",
    "    api_key: str = Field(\n",
    "        description=\"The API key to use for the LLM service.\",\n",
    "        default_factory=lambda: os.getenv(\"OPENAI_API_KEY\"), \n",
    "    )\n",
    "    \n",
    "    api_base: str | None = Field(\n",
    "        description=\"The base URL for the LLM API.\",\n",
    "        default_factory=lambda: os.getenv(\"OPENAI_API_BASE\"), \n",
    "    )\n",
    "    \n",
    "    api_version: str | None = Field(\n",
    "        description=\"The version of the LLM API to use.\",\n",
    "        default_factory=lambda: os.getenv(\"OPEN_AI_VERSION\"), \n",
    "    )\n",
    "    \n",
    "    deployment_name: str | None = Field(\n",
    "        description=\"The deployment name to use for the LLM service.\",\n",
    "        default_factory=lambda: os.getenv(\"GPT_DEPLOYMENT_NAME\"),  \n",
    "    )\n",
    "    ############################\n",
    "    \n",
    "    temperature: float | None = Field(\n",
    "        description=\"The temperature to use for token generation.\",\n",
    "        default=0,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devtools import pformat\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from llm.config.enum import LLMType\n",
    "from llm.config.llm_parameters import LLMParameters\n",
    "\n",
    "class LLMConfig(BaseModel):\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Get a string representation.\"\"\"\n",
    "        return pformat(self, highlight=False)\n",
    "    \n",
    "    type: LLMType = Field(\n",
    "        description='The type of LLM model to sue', default=LLMType.AzureOpenAIChat\n",
    "    )\n",
    "    \n",
    "    llm: LLMParameters = Field(\n",
    "        description=\"The LLM configuration to use.\", default=LLMParameters()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LLMConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://yfyai-aoai-instance.openai.azure.com/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.llm.api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestC:\n",
    "    def __init__(self, config: LLMConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def print_config(self):\n",
    "        return print(self.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testc = TestC(config=LLMConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs.yaml', 'r') as f:\n",
    "    config_data = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usegpu': True,\n",
       " 'seed': 42,\n",
       " 'batch_size': 300,\n",
       " 'lr': '1e-3',\n",
       " 'n_hidden': [16, 32, 32, 16],\n",
       " 'epochs': 100,\n",
       " 'N_fold': 5,\n",
       " 'optimizer': 'Adam',\n",
       " 'weight_decay': '1e-4'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data['Training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_from_file(file_path: str) -> LLMConfig:\n",
    "    \"\"\"Load LLM configuration from a YAML file.\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        config_data = yaml.safe_load(file)\n",
    "    return LLMConfig(**config_data.get(\"LLM\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = load_config_from_file('configs.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMConfig(\n",
       "    type='azure_openai_chat',\n",
       "    llm=LLMParameters(\n",
       "        api_key='c60459f1ee6b4182b16c146727a3112a',\n",
       "        api_base='https://yfyai-aoai-instance.openai.azure.com/',\n",
       "        api_version='2024-04-01-preview',\n",
       "        deployment_name='gpt4o',\n",
       "        temperature=0,\n",
       "    ),\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from predictor.config.ml_configs import NNhyperparameters\n",
    "import yaml\n",
    "\n",
    "class ConfigType(Enum):\n",
    "\n",
    "    LLM = 'llm'\n",
    "    Model_NN = 'Model_NN'\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"'{self.value.lower()}'\"\n",
    "\n",
    "\n",
    "def normalize_keys(d):\n",
    "    if isinstance(d, dict):\n",
    "        return {k.lower(): normalize_keys(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "class YamlLoader:\n",
    "    def __init__(self, file_path):\n",
    "        self.config_data = self.load_config_from_file(file_path)\n",
    "        print(self.config_data)\n",
    "        \n",
    "    def load_config_from_file(self, file_path: str):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            config_data = yaml.safe_load(file)\n",
    "        return normalize_keys(config_data)\n",
    "    \n",
    "    def get_llm_config(self):\n",
    "        return LLMConfig(**self.config_data.get(ConfigType.LLM, {}))\n",
    "    \n",
    "    def get_model_nn_config(self):\n",
    "        return NNhyperparameters(**self.config_data.get(ConfigType.Model_NN, {}))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'futuredataname': 'Bleached Softwood Kraft Pulp Futures Historical Data.csv', 'llm': {'llm_type': 'azure_openai_chat', 'temperature': 0}, 'model_nn': {'seed': 42, 'batch_size': 300, 'lr': '1e-3', 'n_hidden': [16, 32, 32, 16], 'epochs': 100, 'n_fold': 5, 'optimizer': 'Adam', 'weight_decay': '1e-4', 'accelerator': 'gpu'}}\n"
     ]
    }
   ],
   "source": [
    "yaml_config = YamlLoader('configs.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMConfig(\n",
       "    type='azure_openai_chat',\n",
       "    llm=LLMParameters(\n",
       "        api_key='c60459f1ee6b4182b16c146727a3112a',\n",
       "        api_base='https://yfyai-aoai-instance.openai.azure.com/',\n",
       "        api_version='2024-04-01-preview',\n",
       "        deployment_name='gpt4o',\n",
       "        temperature=0,\n",
       "    ),\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_config.get_llm_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bleached Softwood Kraft Pulp Futures Historical Data.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_config.config_data['futuredataname']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
