{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class LLMParameters(BaseModel):\n",
    "    \"\"\"LLM Parameters model.\"\"\"\n",
    "    \n",
    "    ############################ \n",
    "    # Load from .env by default\n",
    "    api_key: str = Field(\n",
    "        description=\"The API key to use for the LLM service.\",\n",
    "        default_factory=lambda: os.getenv(\"OPENAI_API_KEY\"), \n",
    "    )\n",
    "    \n",
    "    api_base: str | None = Field(\n",
    "        description=\"The base URL for the LLM API.\",\n",
    "        default_factory=lambda: os.getenv(\"OPENAI_API_BASE\"), \n",
    "    )\n",
    "    \n",
    "    api_version: str | None = Field(\n",
    "        description=\"The version of the LLM API to use.\",\n",
    "        default_factory=lambda: os.getenv(\"OPEN_AI_VERSION\"), \n",
    "    )\n",
    "    \n",
    "    deployment_name: str | None = Field(\n",
    "        description=\"The deployment name to use for the LLM service.\",\n",
    "        default_factory=lambda: os.getenv(\"GPT_DEPLOYMENT_NAME\"),  \n",
    "    )\n",
    "    ############################\n",
    "    \n",
    "    temperature: float | None = Field(\n",
    "        description=\"The temperature to use for token generation.\",\n",
    "        default=0,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devtools import pformat\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from llm.config.enum import LLMType\n",
    "from llm.config.llm_parameters import LLMParameters\n",
    "\n",
    "class LLMConfig(BaseModel):\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Get a string representation.\"\"\"\n",
    "        return pformat(self, highlight=False)\n",
    "    \n",
    "    type: LLMType = Field(\n",
    "        description='The type of LLM model to sue', default=LLMType.AzureOpenAIChat\n",
    "    )\n",
    "    \n",
    "    llm: LLMParameters = Field(\n",
    "        description=\"The LLM configuration to use.\", default=LLMParameters()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LLMConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://yfyai-aoai-instance.openai.azure.com/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.llm.api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestC:\n",
    "    def __init__(self, config: LLMConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def print_config(self):\n",
    "        return print(self.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testc = TestC(config=LLMConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://yfyai-aoai-instance.openai.azure.com/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testc.config.llm.api_base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
